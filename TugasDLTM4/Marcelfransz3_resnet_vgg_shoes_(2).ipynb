{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492d9d2f",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:23.749075Z",
          "iopub.status.busy": "2024-01-20T18:36:23.748204Z",
          "iopub.status.idle": "2024-01-20T18:36:24.545383Z",
          "shell.execute_reply": "2024-01-20T18:36:24.544422Z"
        },
        "papermill": {
          "duration": 0.807119,
          "end_time": "2024-01-20T18:36:24.547822",
          "exception": false,
          "start_time": "2024-01-20T18:36:23.740703",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "492d9d2f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92029576",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:24.559780Z",
          "iopub.status.busy": "2024-01-20T18:36:24.559345Z",
          "iopub.status.idle": "2024-01-20T18:36:38.050928Z",
          "shell.execute_reply": "2024-01-20T18:36:38.050067Z"
        },
        "papermill": {
          "duration": 13.499905,
          "end_time": "2024-01-20T18:36:38.053262",
          "exception": false,
          "start_time": "2024-01-20T18:36:24.553357",
          "status": "completed"
        },
        "tags": [],
        "id": "92029576"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF9teXqLnzEQ",
        "outputId": "7d9f5238-a7a3-4fb3-83f2-5b59cfb2c326"
      },
      "id": "YF9teXqLnzEQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da495ece",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:38.065445Z",
          "iopub.status.busy": "2024-01-20T18:36:38.064498Z",
          "iopub.status.idle": "2024-01-20T18:36:38.074594Z",
          "shell.execute_reply": "2024-01-20T18:36:38.073729Z"
        },
        "papermill": {
          "duration": 0.017837,
          "end_time": "2024-01-20T18:36:38.076418",
          "exception": false,
          "start_time": "2024-01-20T18:36:38.058581",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da495ece",
        "outputId": "be0bafe6-bb85-4825-e480-2e173b8c1ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2\n",
            "Target Names: ['nike', 'adidas']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to your dataset\n",
        "fpath = \"/content/drive/My Drive/archive/train\"\n",
        "dataset_path = fpath\n",
        "\n",
        "# Get a list of subdirectories (each subdirectory corresponds to a class)\n",
        "class_folders = [f.name for f in os.scandir(dataset_path) if f.is_dir()]\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = len(class_folders)\n",
        "\n",
        "# Print the number of classes\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "# Create a list of target_names based on the directory names\n",
        "target_names = class_folders\n",
        "print(\"Target Names:\", target_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1cb27f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:38.087901Z",
          "iopub.status.busy": "2024-01-20T18:36:38.087590Z",
          "iopub.status.idle": "2024-01-20T18:36:38.091947Z",
          "shell.execute_reply": "2024-01-20T18:36:38.091078Z"
        },
        "papermill": {
          "duration": 0.01219,
          "end_time": "2024-01-20T18:36:38.094014",
          "exception": false,
          "start_time": "2024-01-20T18:36:38.081824",
          "status": "completed"
        },
        "tags": [],
        "id": "6d1cb27f"
      },
      "outputs": [],
      "source": [
        "img_size = 224\n",
        "batch_size =64\n",
        "fpath = \"/content/drive/My Drive/archive/train\"\n",
        "ffpath=\"/content/drive/My Drive/archive/test\"\n",
        "fffpath=\"/content/drive/My Drive/archive/validation\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee308278",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:38.105223Z",
          "iopub.status.busy": "2024-01-20T18:36:38.104962Z",
          "iopub.status.idle": "2024-01-20T18:36:38.171852Z",
          "shell.execute_reply": "2024-01-20T18:36:38.171106Z"
        },
        "papermill": {
          "duration": 0.074937,
          "end_time": "2024-01-20T18:36:38.173816",
          "exception": false,
          "start_time": "2024-01-20T18:36:38.098879",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee308278",
        "outputId": "de87bb84-fa78-4c06-f5aa-40d03844cf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 460 images belonging to 2 classes.\n",
            "Found 55 images belonging to 2 classes.\n",
            "Found 60 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Assuming 'fpath' is the path to your dataset and 'batch_size' is defined\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=30,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    fpath,\n",
        "    target_size=(224, 224),\n",
        "    subset=\"training\",\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_gen = datagen.flow_from_directory(\n",
        "    fffpath,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=(batch_size),\n",
        "    subset='training',\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# For test data\n",
        "test_gen = ImageDataGenerator(rescale=1/255.).flow_from_directory(\n",
        "    ffpath,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Set shuffle to False for test data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac96fe89",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-01-20T18:36:38.187466Z",
          "iopub.status.busy": "2024-01-20T18:36:38.187004Z",
          "iopub.status.idle": "2024-01-20T18:39:31.594387Z",
          "shell.execute_reply": "2024-01-20T18:39:31.593479Z"
        },
        "papermill": {
          "duration": 173.41614,
          "end_time": "2024-01-20T18:39:31.596526",
          "exception": false,
          "start_time": "2024-01-20T18:36:38.180386",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac96fe89",
        "outputId": "8212850f-484d-4142-c1b4-c79164e0e7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/8 [======>.......................] - ETA: 3s - loss: 1.0489 - accuracy: 0.5263  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 1.1258 - accuracy: 0.4783\n",
            "Epoch 1: val_accuracy improved from -inf to 0.49091, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 27s 2s/step - loss: 1.1258 - accuracy: 0.4783 - val_loss: 0.7431 - val_accuracy: 0.4909\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.4537 - accuracy: 0.4674\n",
            "Epoch 2: val_accuracy improved from 0.49091 to 0.50909, saving model to best_model.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 1.4537 - accuracy: 0.4674 - val_loss: 2.5516 - val_accuracy: 0.5091\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.5065\n",
            "Epoch 3: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 1.0732 - accuracy: 0.5065 - val_loss: 1.0620 - val_accuracy: 0.4909\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0536 - accuracy: 0.5152\n",
            "Epoch 4: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 1.0536 - accuracy: 0.5152 - val_loss: 0.9699 - val_accuracy: 0.4909\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9716 - accuracy: 0.4696\n",
            "Epoch 5: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.9716 - accuracy: 0.4696 - val_loss: 0.7309 - val_accuracy: 0.4909\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.4913\n",
            "Epoch 6: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.9019 - accuracy: 0.4913 - val_loss: 0.7125 - val_accuracy: 0.4909\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9154 - accuracy: 0.4609\n",
            "Epoch 7: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.9154 - accuracy: 0.4609 - val_loss: 0.7018 - val_accuracy: 0.5091\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7963 - accuracy: 0.5109\n",
            "Epoch 8: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7963 - accuracy: 0.5109 - val_loss: 0.6981 - val_accuracy: 0.5091\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7794 - accuracy: 0.5152\n",
            "Epoch 9: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7794 - accuracy: 0.5152 - val_loss: 0.6958 - val_accuracy: 0.5091\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7515 - accuracy: 0.5022\n",
            "Epoch 10: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7515 - accuracy: 0.5022 - val_loss: 0.6948 - val_accuracy: 0.4909\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.4891\n",
            "Epoch 11: val_accuracy improved from 0.50909 to 0.52727, saving model to best_model.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7420 - accuracy: 0.4891 - val_loss: 0.6915 - val_accuracy: 0.5273\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7120 - accuracy: 0.5087\n",
            "Epoch 12: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7120 - accuracy: 0.5087 - val_loss: 0.6950 - val_accuracy: 0.4909\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7656 - accuracy: 0.4935\n",
            "Epoch 13: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7656 - accuracy: 0.4935 - val_loss: 0.7207 - val_accuracy: 0.4909\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7134 - accuracy: 0.5109\n",
            "Epoch 14: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7134 - accuracy: 0.5109 - val_loss: 0.6997 - val_accuracy: 0.4909\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7458 - accuracy: 0.4891\n",
            "Epoch 15: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7458 - accuracy: 0.4891 - val_loss: 0.6987 - val_accuracy: 0.4909\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.5000\n",
            "Epoch 16: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7148 - accuracy: 0.5000 - val_loss: 0.7006 - val_accuracy: 0.4909\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.4957\n",
            "Epoch 17: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7214 - accuracy: 0.4957 - val_loss: 0.6974 - val_accuracy: 0.4909\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.5457\n",
            "Epoch 18: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6891 - accuracy: 0.5457 - val_loss: 0.6963 - val_accuracy: 0.4909\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.4848\n",
            "Epoch 19: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7022 - accuracy: 0.4848 - val_loss: 0.6953 - val_accuracy: 0.4000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.5196\n",
            "Epoch 20: val_accuracy improved from 0.52727 to 0.58182, saving model to best_model.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7167 - accuracy: 0.5196 - val_loss: 0.6932 - val_accuracy: 0.5818\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7172 - accuracy: 0.4761\n",
            "Epoch 21: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7172 - accuracy: 0.4761 - val_loss: 0.6931 - val_accuracy: 0.4727\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.4826\n",
            "Epoch 22: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7226 - accuracy: 0.4826 - val_loss: 0.6927 - val_accuracy: 0.5636\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.5217\n",
            "Epoch 23: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6951 - accuracy: 0.5217 - val_loss: 0.6929 - val_accuracy: 0.5091\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.5022\n",
            "Epoch 24: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7266 - accuracy: 0.5022 - val_loss: 0.6926 - val_accuracy: 0.5455\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.4913\n",
            "Epoch 25: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7090 - accuracy: 0.4913 - val_loss: 0.6930 - val_accuracy: 0.5091\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5674\n",
            "Epoch 26: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6935 - accuracy: 0.5674 - val_loss: 0.6931 - val_accuracy: 0.5091\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5348\n",
            "Epoch 27: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6932 - accuracy: 0.5348 - val_loss: 0.6930 - val_accuracy: 0.5273\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.4891\n",
            "Epoch 28: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7010 - accuracy: 0.4891 - val_loss: 0.6927 - val_accuracy: 0.5455\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.5152\n",
            "Epoch 29: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7023 - accuracy: 0.5152 - val_loss: 0.6930 - val_accuracy: 0.4909\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.4891\n",
            "Epoch 30: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6981 - accuracy: 0.4891 - val_loss: 0.6937 - val_accuracy: 0.4909\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.5109\n",
            "Epoch 31: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6973 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.4909\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6955 - accuracy: 0.5283\n",
            "Epoch 32: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6955 - accuracy: 0.5283 - val_loss: 0.6937 - val_accuracy: 0.4909\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5304\n",
            "Epoch 33: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6934 - accuracy: 0.5304 - val_loss: 0.6928 - val_accuracy: 0.4909\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.5326\n",
            "Epoch 34: val_accuracy did not improve from 0.58182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6964 - accuracy: 0.5326 - val_loss: 0.6926 - val_accuracy: 0.4909\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.5043\n",
            "Epoch 35: val_accuracy improved from 0.58182 to 0.60000, saving model to best_model.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7022 - accuracy: 0.5043 - val_loss: 0.6915 - val_accuracy: 0.6000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.4957\n",
            "Epoch 36: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7057 - accuracy: 0.4957 - val_loss: 0.6930 - val_accuracy: 0.5273\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.5109\n",
            "Epoch 37: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7024 - accuracy: 0.5109 - val_loss: 0.6955 - val_accuracy: 0.5455\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.4913\n",
            "Epoch 38: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6962 - accuracy: 0.4913 - val_loss: 0.6932 - val_accuracy: 0.5273\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.4978\n",
            "Epoch 39: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6911 - accuracy: 0.4978 - val_loss: 0.6944 - val_accuracy: 0.4909\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.5043\n",
            "Epoch 40: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6887 - accuracy: 0.5043 - val_loss: 0.6939 - val_accuracy: 0.5091\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.5022\n",
            "Epoch 41: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7031 - accuracy: 0.5022 - val_loss: 0.6944 - val_accuracy: 0.4727\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.5022\n",
            "Epoch 42: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7047 - accuracy: 0.5022 - val_loss: 0.6950 - val_accuracy: 0.4182\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.5065\n",
            "Epoch 43: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7137 - accuracy: 0.5065 - val_loss: 0.6937 - val_accuracy: 0.4364\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.5022\n",
            "Epoch 44: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6957 - accuracy: 0.5022 - val_loss: 0.6935 - val_accuracy: 0.4545\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.5196\n",
            "Epoch 45: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7063 - accuracy: 0.5196 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7033 - accuracy: 0.5348\n",
            "Epoch 46: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7033 - accuracy: 0.5348 - val_loss: 0.6933 - val_accuracy: 0.4727\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7019 - accuracy: 0.4913\n",
            "Epoch 47: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7019 - accuracy: 0.4913 - val_loss: 0.6897 - val_accuracy: 0.5091\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7107 - accuracy: 0.4978\n",
            "Epoch 48: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7107 - accuracy: 0.4978 - val_loss: 0.6931 - val_accuracy: 0.4909\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.5326\n",
            "Epoch 49: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7007 - accuracy: 0.5326 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7038 - accuracy: 0.4957\n",
            "Epoch 50: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7038 - accuracy: 0.4957 - val_loss: 0.6948 - val_accuracy: 0.4909\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.4717\n",
            "Epoch 51: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6975 - accuracy: 0.4717 - val_loss: 0.6919 - val_accuracy: 0.5455\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.4630\n",
            "Epoch 52: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6958 - accuracy: 0.4630 - val_loss: 0.6930 - val_accuracy: 0.5091\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4522\n",
            "Epoch 53: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6938 - accuracy: 0.4522 - val_loss: 0.6936 - val_accuracy: 0.4364\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.4870\n",
            "Epoch 54: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6956 - accuracy: 0.4870 - val_loss: 0.6919 - val_accuracy: 0.4909\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7018 - accuracy: 0.5283\n",
            "Epoch 55: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7018 - accuracy: 0.5283 - val_loss: 0.6919 - val_accuracy: 0.5273\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.5326\n",
            "Epoch 56: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6973 - accuracy: 0.5326 - val_loss: 0.6928 - val_accuracy: 0.4182\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.4891\n",
            "Epoch 57: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6949 - accuracy: 0.4891 - val_loss: 0.6937 - val_accuracy: 0.4545\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.5304\n",
            "Epoch 58: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6923 - accuracy: 0.5304 - val_loss: 0.6933 - val_accuracy: 0.4727\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.4717\n",
            "Epoch 59: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7095 - accuracy: 0.4717 - val_loss: 0.6933 - val_accuracy: 0.5818\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5283\n",
            "Epoch 60: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6924 - accuracy: 0.5283 - val_loss: 0.6982 - val_accuracy: 0.4909\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5239\n",
            "Epoch 61: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6943 - accuracy: 0.5239 - val_loss: 0.6916 - val_accuracy: 0.6000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.4630\n",
            "Epoch 62: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7042 - accuracy: 0.4630 - val_loss: 0.6935 - val_accuracy: 0.4909\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.4652\n",
            "Epoch 63: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6926 - accuracy: 0.4652 - val_loss: 0.6929 - val_accuracy: 0.4909\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.4978\n",
            "Epoch 64: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6948 - accuracy: 0.4978 - val_loss: 0.6975 - val_accuracy: 0.4727\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4717\n",
            "Epoch 65: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6934 - accuracy: 0.4717 - val_loss: 0.6927 - val_accuracy: 0.4727\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.5043\n",
            "Epoch 66: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6963 - accuracy: 0.5043 - val_loss: 0.6974 - val_accuracy: 0.3818\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.5109\n",
            "Epoch 67: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6956 - accuracy: 0.5109 - val_loss: 0.6938 - val_accuracy: 0.5091\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6988 - accuracy: 0.4826\n",
            "Epoch 68: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6988 - accuracy: 0.4826 - val_loss: 0.6909 - val_accuracy: 0.4909\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.5348\n",
            "Epoch 69: val_accuracy improved from 0.60000 to 0.61818, saving model to best_model.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7076 - accuracy: 0.5348 - val_loss: 0.6930 - val_accuracy: 0.6182\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.4413\n",
            "Epoch 70: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6973 - accuracy: 0.4413 - val_loss: 0.6949 - val_accuracy: 0.4909\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.5022\n",
            "Epoch 71: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6946 - accuracy: 0.5022 - val_loss: 0.6921 - val_accuracy: 0.4182\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5261\n",
            "Epoch 72: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6934 - accuracy: 0.5261 - val_loss: 0.6924 - val_accuracy: 0.4909\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5152\n",
            "Epoch 73: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.6934 - accuracy: 0.5152 - val_loss: 0.6950 - val_accuracy: 0.4909\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7183 - accuracy: 0.4978\n",
            "Epoch 74: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7183 - accuracy: 0.4978 - val_loss: 0.6934 - val_accuracy: 0.4909\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.5065\n",
            "Epoch 75: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6963 - accuracy: 0.5065 - val_loss: 0.6939 - val_accuracy: 0.4727\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4870\n",
            "Epoch 76: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6942 - accuracy: 0.4870 - val_loss: 0.6935 - val_accuracy: 0.4727\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.5065\n",
            "Epoch 77: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6956 - accuracy: 0.5065 - val_loss: 0.6942 - val_accuracy: 0.4545\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4826\n",
            "Epoch 78: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6941 - accuracy: 0.4826 - val_loss: 0.6950 - val_accuracy: 0.4545\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.4891\n",
            "Epoch 79: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6946 - accuracy: 0.4891 - val_loss: 0.6909 - val_accuracy: 0.5455\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5043\n",
            "Epoch 80: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6920 - accuracy: 0.5043 - val_loss: 0.6944 - val_accuracy: 0.5091\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.4652\n",
            "Epoch 81: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6956 - accuracy: 0.4652 - val_loss: 0.6948 - val_accuracy: 0.4545\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.4935\n",
            "Epoch 82: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6917 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5273\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4717\n",
            "Epoch 83: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6943 - accuracy: 0.4717 - val_loss: 0.6935 - val_accuracy: 0.4909\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.5130\n",
            "Epoch 84: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6916 - accuracy: 0.5130 - val_loss: 0.6935 - val_accuracy: 0.4545\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.4891\n",
            "Epoch 85: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.7037 - accuracy: 0.4891 - val_loss: 0.6938 - val_accuracy: 0.4364\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.5130\n",
            "Epoch 86: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6893 - accuracy: 0.5130 - val_loss: 0.6925 - val_accuracy: 0.5273\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4870\n",
            "Epoch 87: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6934 - accuracy: 0.4870 - val_loss: 0.6929 - val_accuracy: 0.5091\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6966 - accuracy: 0.4935\n",
            "Epoch 88: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6966 - accuracy: 0.4935 - val_loss: 0.6927 - val_accuracy: 0.4727\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.4848\n",
            "Epoch 89: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6952 - accuracy: 0.4848 - val_loss: 0.6933 - val_accuracy: 0.4364\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.4913\n",
            "Epoch 90: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7022 - accuracy: 0.4913 - val_loss: 0.6924 - val_accuracy: 0.4909\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.4913\n",
            "Epoch 91: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6912 - accuracy: 0.4913 - val_loss: 0.6947 - val_accuracy: 0.4364\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.4522\n",
            "Epoch 92: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6952 - accuracy: 0.4522 - val_loss: 0.6927 - val_accuracy: 0.5636\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.4913\n",
            "Epoch 93: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6944 - accuracy: 0.4913 - val_loss: 0.6927 - val_accuracy: 0.4909\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7099 - accuracy: 0.5109\n",
            "Epoch 94: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7099 - accuracy: 0.5109 - val_loss: 0.6924 - val_accuracy: 0.4545\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4957\n",
            "Epoch 95: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6934 - accuracy: 0.4957 - val_loss: 0.6896 - val_accuracy: 0.5273\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.5348\n",
            "Epoch 96: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6917 - accuracy: 0.5348 - val_loss: 0.6931 - val_accuracy: 0.5273\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.5109\n",
            "Epoch 97: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6992 - accuracy: 0.5109 - val_loss: 0.6931 - val_accuracy: 0.5091\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.4957\n",
            "Epoch 98: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6944 - accuracy: 0.4957 - val_loss: 0.6924 - val_accuracy: 0.5273\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.4935\n",
            "Epoch 99: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6956 - accuracy: 0.4935 - val_loss: 0.6932 - val_accuracy: 0.4909\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.5500\n",
            "Epoch 100: val_accuracy did not improve from 0.61818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.6915 - accuracy: 0.5500 - val_loss: 0.6929 - val_accuracy: 0.5091\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Assuming train_gen and valid_gen are your image data generators\n",
        "\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys()))\n",
        "\n",
        "# Create pre-trained model\n",
        "base_model = tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\", input_shape=img_shape, pooling='max')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "     Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define filepath to save the best model\n",
        "filepath = 'best_model.h5'\n",
        "\n",
        "# Create ModelCheckpoint callback to save the best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "# Train the model with the added callback\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_gen,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=valid_gen,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a304b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:39:31.634721Z",
          "iopub.status.busy": "2024-01-20T18:39:31.634392Z",
          "iopub.status.idle": "2024-01-20T18:39:51.302048Z",
          "shell.execute_reply": "2024-01-20T18:39:51.300806Z"
        },
        "papermill": {
          "duration": 19.688717,
          "end_time": "2024-01-20T18:39:51.304103",
          "exception": false,
          "start_time": "2024-01-20T18:39:31.615386",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a304b8",
        "outputId": "c5a45e4b-93c3-408e-8e45-36f020010730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 11s 998ms/step - loss: 0.6908 - accuracy: 0.5587\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6929 - accuracy: 0.4909\n",
            "1/1 [==============================] - 1s 611ms/step - loss: 0.6893 - accuracy: 0.6333\n",
            "Train Loss:  0.6908228397369385\n",
            "Train Accuracy:  0.5586956739425659\n",
            "--------------------\n",
            "Validation Loss:  0.692883312702179\n",
            "Validation Accuracy:  0.4909090995788574\n",
            "--------------------\n",
            "Test Loss:  0.689257800579071\n",
            "Test Accuracy:  0.6333333253860474\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "\n",
        "\n",
        "\n",
        "model= load_model('best_model.h5')\n",
        "\n",
        "train_score = model.evaluate(train_gen, steps= len(train_gen), verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= len(valid_gen), verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= len(test_gen), verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6fa145",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:39:51.344215Z",
          "iopub.status.busy": "2024-01-20T18:39:51.343904Z",
          "iopub.status.idle": "2024-01-20T18:39:59.368223Z",
          "shell.execute_reply": "2024-01-20T18:39:59.367156Z"
        },
        "papermill": {
          "duration": 8.046661,
          "end_time": "2024-01-20T18:39:59.370503",
          "exception": false,
          "start_time": "2024-01-20T18:39:51.323842",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d6fa145",
        "outputId": "d9e9dbf9-245e-4a89-b6c2-915caa72dedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-c704b8641e8f>:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  preds = model.predict_generator(test_gen)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "model = load_model('best_model.h5')\n",
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4854a36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:39:59.412072Z",
          "iopub.status.busy": "2024-01-20T18:39:59.411427Z",
          "iopub.status.idle": "2024-01-20T18:39:59.420629Z",
          "shell.execute_reply": "2024-01-20T18:39:59.419825Z"
        },
        "papermill": {
          "duration": 0.031323,
          "end_time": "2024-01-20T18:39:59.422423",
          "exception": false,
          "start_time": "2024-01-20T18:39:59.391100",
          "status": "completed"
        },
        "tags": [],
        "id": "d4854a36"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
        "    plt.figure(figsize= (10, 10))\n",
        "    plt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation= 45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
        "        print('Normalized Confusion Matrix')\n",
        "    else:\n",
        "        print('Confusion Matrix, Without Normalization')\n",
        "    print(cm)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f29d31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:39:59.462625Z",
          "iopub.status.busy": "2024-01-20T18:39:59.461950Z",
          "iopub.status.idle": "2024-01-20T18:39:59.955719Z",
          "shell.execute_reply": "2024-01-20T18:39:59.954660Z"
        },
        "papermill": {
          "duration": 0.516353,
          "end_time": "2024-01-20T18:39:59.958211",
          "exception": false,
          "start_time": "2024-01-20T18:39:59.441858",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "39f29d31",
        "outputId": "87ec036a-f5e7-4044-fcd9-b8cf21f3dbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix, Without Normalization\n",
            "[[16 14]\n",
            " [ 8 22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        nike       0.67      0.53      0.59        30\n",
            "      adidas       0.61      0.73      0.67        30\n",
            "\n",
            "    accuracy                           0.63        60\n",
            "   macro avg       0.64      0.63      0.63        60\n",
            "weighted avg       0.64      0.63      0.63        60\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0706cb",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-01-20T18:39:59.999537Z",
          "iopub.status.busy": "2024-01-20T18:39:59.999250Z",
          "iopub.status.idle": "2024-01-20T18:42:42.244332Z",
          "shell.execute_reply": "2024-01-20T18:42:42.243340Z"
        },
        "papermill": {
          "duration": 162.268042,
          "end_time": "2024-01-20T18:42:42.246450",
          "exception": false,
          "start_time": "2024-01-20T18:39:59.978408",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0706cb",
        "outputId": "2d60da5c-3c1c-4b6a-846e-42b1ba188701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/8 [==============>...............] - ETA: 3s - loss: 0.8615 - accuracy: 0.5039"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 0.8026 - accuracy: 0.5696\n",
            "Epoch 1: val_accuracy improved from -inf to 0.49091, saving model to best_model_resnet.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 17s 2s/step - loss: 0.8026 - accuracy: 0.5696 - val_loss: 1.6890 - val_accuracy: 0.4909\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.5957\n",
            "Epoch 2: val_accuracy improved from 0.49091 to 0.50909, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7549 - accuracy: 0.5957 - val_loss: 0.9241 - val_accuracy: 0.5091\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.6935\n",
            "Epoch 3: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.6461 - accuracy: 0.6935 - val_loss: 1.3931 - val_accuracy: 0.5091\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.6587\n",
            "Epoch 4: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6402 - accuracy: 0.6587 - val_loss: 1.2007 - val_accuracy: 0.4909\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.6565\n",
            "Epoch 5: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.6219 - accuracy: 0.6565 - val_loss: 1.2459 - val_accuracy: 0.4909\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7283\n",
            "Epoch 6: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.5551 - accuracy: 0.7283 - val_loss: 1.1936 - val_accuracy: 0.4909\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7413\n",
            "Epoch 7: val_accuracy did not improve from 0.50909\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.5980 - accuracy: 0.7413 - val_loss: 1.1156 - val_accuracy: 0.5091\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.6957\n",
            "Epoch 8: val_accuracy improved from 0.50909 to 0.52727, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.5692 - accuracy: 0.6957 - val_loss: 0.8156 - val_accuracy: 0.5273\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7217\n",
            "Epoch 9: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.5545 - accuracy: 0.7217 - val_loss: 0.7462 - val_accuracy: 0.5091\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.7217\n",
            "Epoch 10: val_accuracy did not improve from 0.52727\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.5287 - accuracy: 0.7217 - val_loss: 0.6867 - val_accuracy: 0.4909\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.7391\n",
            "Epoch 11: val_accuracy improved from 0.52727 to 0.60000, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.5216 - accuracy: 0.7391 - val_loss: 0.6655 - val_accuracy: 0.6000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.7152\n",
            "Epoch 12: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.5158 - accuracy: 0.7152 - val_loss: 0.7119 - val_accuracy: 0.5455\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.7630\n",
            "Epoch 13: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4786 - accuracy: 0.7630 - val_loss: 0.7090 - val_accuracy: 0.5273\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.7304\n",
            "Epoch 14: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.5178 - accuracy: 0.7304 - val_loss: 0.7504 - val_accuracy: 0.5273\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4963 - accuracy: 0.7761\n",
            "Epoch 15: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4963 - accuracy: 0.7761 - val_loss: 0.8608 - val_accuracy: 0.4909\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.7565\n",
            "Epoch 16: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.5023 - accuracy: 0.7565 - val_loss: 0.7484 - val_accuracy: 0.5273\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.7457\n",
            "Epoch 17: val_accuracy did not improve from 0.60000\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4864 - accuracy: 0.7457 - val_loss: 0.7718 - val_accuracy: 0.4727\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.7826\n",
            "Epoch 18: val_accuracy improved from 0.60000 to 0.63636, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.4508 - accuracy: 0.7826 - val_loss: 0.6528 - val_accuracy: 0.6364\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.7957\n",
            "Epoch 19: val_accuracy improved from 0.63636 to 0.72727, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.4550 - accuracy: 0.7957 - val_loss: 0.5652 - val_accuracy: 0.7273\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.7804\n",
            "Epoch 20: val_accuracy did not improve from 0.72727\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.4361 - accuracy: 0.7804 - val_loss: 0.6804 - val_accuracy: 0.5818\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.7652\n",
            "Epoch 21: val_accuracy improved from 0.72727 to 0.78182, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4976 - accuracy: 0.7652 - val_loss: 0.5035 - val_accuracy: 0.7818\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.7804\n",
            "Epoch 22: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4802 - accuracy: 0.7804 - val_loss: 0.5794 - val_accuracy: 0.6727\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.7913\n",
            "Epoch 23: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.4662 - accuracy: 0.7913 - val_loss: 0.5135 - val_accuracy: 0.7818\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7500\n",
            "Epoch 24: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.5032 - accuracy: 0.7500 - val_loss: 0.5472 - val_accuracy: 0.7818\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.7826\n",
            "Epoch 25: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4768 - accuracy: 0.7826 - val_loss: 0.5786 - val_accuracy: 0.6727\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.7696\n",
            "Epoch 26: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.4755 - accuracy: 0.7696 - val_loss: 0.6129 - val_accuracy: 0.6727\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.7761\n",
            "Epoch 27: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4887 - accuracy: 0.7761 - val_loss: 0.6000 - val_accuracy: 0.6364\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.8065\n",
            "Epoch 28: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4387 - accuracy: 0.8065 - val_loss: 0.6335 - val_accuracy: 0.5818\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8043\n",
            "Epoch 29: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4189 - accuracy: 0.8043 - val_loss: 0.6337 - val_accuracy: 0.6000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.7609\n",
            "Epoch 30: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.4706 - accuracy: 0.7609 - val_loss: 0.5868 - val_accuracy: 0.6727\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8022\n",
            "Epoch 31: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4027 - accuracy: 0.8022 - val_loss: 0.6366 - val_accuracy: 0.7273\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8130\n",
            "Epoch 32: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4107 - accuracy: 0.8130 - val_loss: 0.6230 - val_accuracy: 0.6727\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8348\n",
            "Epoch 33: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3940 - accuracy: 0.8348 - val_loss: 0.5552 - val_accuracy: 0.6727\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8000\n",
            "Epoch 34: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3921 - accuracy: 0.8000 - val_loss: 0.5813 - val_accuracy: 0.6909\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8174\n",
            "Epoch 35: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3671 - accuracy: 0.8174 - val_loss: 0.5970 - val_accuracy: 0.6545\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8304\n",
            "Epoch 36: val_accuracy did not improve from 0.78182\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3676 - accuracy: 0.8304 - val_loss: 0.6046 - val_accuracy: 0.6727\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8457\n",
            "Epoch 37: val_accuracy improved from 0.78182 to 0.81818, saving model to best_model_resnet.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3637 - accuracy: 0.8457 - val_loss: 0.4719 - val_accuracy: 0.8182\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8196\n",
            "Epoch 38: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4136 - accuracy: 0.8196 - val_loss: 0.5962 - val_accuracy: 0.6000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8261\n",
            "Epoch 39: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3901 - accuracy: 0.8261 - val_loss: 0.4993 - val_accuracy: 0.7455\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8413\n",
            "Epoch 40: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3631 - accuracy: 0.8413 - val_loss: 0.5636 - val_accuracy: 0.7091\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8261\n",
            "Epoch 41: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3857 - accuracy: 0.8261 - val_loss: 0.5441 - val_accuracy: 0.6909\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8065\n",
            "Epoch 42: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.4213 - accuracy: 0.8065 - val_loss: 0.6246 - val_accuracy: 0.6364\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.8522\n",
            "Epoch 43: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3339 - accuracy: 0.8522 - val_loss: 0.5531 - val_accuracy: 0.7636\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.8152\n",
            "Epoch 44: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.3928 - accuracy: 0.8152 - val_loss: 0.4907 - val_accuracy: 0.7636\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8304\n",
            "Epoch 45: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3803 - accuracy: 0.8304 - val_loss: 0.5964 - val_accuracy: 0.6727\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8565\n",
            "Epoch 46: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3257 - accuracy: 0.8565 - val_loss: 0.6831 - val_accuracy: 0.7273\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8283\n",
            "Epoch 47: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3759 - accuracy: 0.8283 - val_loss: 0.5925 - val_accuracy: 0.7091\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8543\n",
            "Epoch 48: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3573 - accuracy: 0.8543 - val_loss: 0.6355 - val_accuracy: 0.7273\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8457\n",
            "Epoch 49: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3554 - accuracy: 0.8457 - val_loss: 0.6018 - val_accuracy: 0.7636\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8370\n",
            "Epoch 50: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3412 - accuracy: 0.8370 - val_loss: 0.6468 - val_accuracy: 0.7091\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8348\n",
            "Epoch 51: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3639 - accuracy: 0.8348 - val_loss: 0.7008 - val_accuracy: 0.7273\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8457\n",
            "Epoch 52: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3653 - accuracy: 0.8457 - val_loss: 0.6781 - val_accuracy: 0.6909\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.8457\n",
            "Epoch 53: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3574 - accuracy: 0.8457 - val_loss: 0.6275 - val_accuracy: 0.7455\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8457\n",
            "Epoch 54: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3454 - accuracy: 0.8457 - val_loss: 0.5282 - val_accuracy: 0.6909\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8500\n",
            "Epoch 55: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3561 - accuracy: 0.8500 - val_loss: 0.6564 - val_accuracy: 0.6909\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.8304\n",
            "Epoch 56: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3716 - accuracy: 0.8304 - val_loss: 0.7367 - val_accuracy: 0.7273\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8543\n",
            "Epoch 57: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3509 - accuracy: 0.8543 - val_loss: 0.6818 - val_accuracy: 0.7091\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8522\n",
            "Epoch 58: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3343 - accuracy: 0.8522 - val_loss: 0.7501 - val_accuracy: 0.6545\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8348\n",
            "Epoch 59: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3726 - accuracy: 0.8348 - val_loss: 0.5446 - val_accuracy: 0.7636\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8413\n",
            "Epoch 60: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3678 - accuracy: 0.8413 - val_loss: 0.5405 - val_accuracy: 0.7636\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8435\n",
            "Epoch 61: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3345 - accuracy: 0.8435 - val_loss: 0.7790 - val_accuracy: 0.6909\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8783\n",
            "Epoch 62: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2856 - accuracy: 0.8783 - val_loss: 0.7322 - val_accuracy: 0.7455\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.8413\n",
            "Epoch 63: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3479 - accuracy: 0.8413 - val_loss: 0.7459 - val_accuracy: 0.6727\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8848\n",
            "Epoch 64: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2923 - accuracy: 0.8848 - val_loss: 0.7583 - val_accuracy: 0.6909\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8674\n",
            "Epoch 65: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3509 - accuracy: 0.8674 - val_loss: 0.7371 - val_accuracy: 0.6545\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8761\n",
            "Epoch 66: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2954 - accuracy: 0.8761 - val_loss: 0.5939 - val_accuracy: 0.7091\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8283\n",
            "Epoch 67: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3806 - accuracy: 0.8283 - val_loss: 0.6466 - val_accuracy: 0.7091\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8326\n",
            "Epoch 68: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3364 - accuracy: 0.8326 - val_loss: 0.9030 - val_accuracy: 0.6364\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8435\n",
            "Epoch 69: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3490 - accuracy: 0.8435 - val_loss: 0.6799 - val_accuracy: 0.6727\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8652\n",
            "Epoch 70: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3455 - accuracy: 0.8652 - val_loss: 0.7200 - val_accuracy: 0.6182\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8674\n",
            "Epoch 71: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3036 - accuracy: 0.8674 - val_loss: 0.6594 - val_accuracy: 0.7273\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8761\n",
            "Epoch 72: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2988 - accuracy: 0.8761 - val_loss: 0.6027 - val_accuracy: 0.7273\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8587\n",
            "Epoch 73: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3291 - accuracy: 0.8587 - val_loss: 0.6514 - val_accuracy: 0.7273\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8457\n",
            "Epoch 74: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3565 - accuracy: 0.8457 - val_loss: 0.6384 - val_accuracy: 0.7091\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8652\n",
            "Epoch 75: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3066 - accuracy: 0.8652 - val_loss: 0.6911 - val_accuracy: 0.6727\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.8891\n",
            "Epoch 76: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2374 - accuracy: 0.8891 - val_loss: 0.6036 - val_accuracy: 0.7091\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8587\n",
            "Epoch 77: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.3094 - accuracy: 0.8587 - val_loss: 0.6687 - val_accuracy: 0.7091\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.8717\n",
            "Epoch 78: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2640 - accuracy: 0.8717 - val_loss: 0.6900 - val_accuracy: 0.6545\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8935\n",
            "Epoch 79: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.2705 - accuracy: 0.8935 - val_loss: 0.9065 - val_accuracy: 0.6364\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8630\n",
            "Epoch 80: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3251 - accuracy: 0.8630 - val_loss: 1.1809 - val_accuracy: 0.6000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.8413\n",
            "Epoch 81: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3311 - accuracy: 0.8413 - val_loss: 0.6704 - val_accuracy: 0.6727\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8957\n",
            "Epoch 82: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2919 - accuracy: 0.8957 - val_loss: 0.7037 - val_accuracy: 0.7273\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8739\n",
            "Epoch 83: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2982 - accuracy: 0.8739 - val_loss: 0.8197 - val_accuracy: 0.6545\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8870\n",
            "Epoch 84: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3077 - accuracy: 0.8870 - val_loss: 0.9071 - val_accuracy: 0.6364\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.8717\n",
            "Epoch 85: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3267 - accuracy: 0.8717 - val_loss: 0.7262 - val_accuracy: 0.6727\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.8783\n",
            "Epoch 86: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2850 - accuracy: 0.8783 - val_loss: 1.0502 - val_accuracy: 0.6909\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8630\n",
            "Epoch 87: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3417 - accuracy: 0.8630 - val_loss: 0.8779 - val_accuracy: 0.6545\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8761\n",
            "Epoch 88: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2666 - accuracy: 0.8761 - val_loss: 0.6639 - val_accuracy: 0.7455\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8804\n",
            "Epoch 89: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3113 - accuracy: 0.8804 - val_loss: 0.7949 - val_accuracy: 0.6182\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.8870\n",
            "Epoch 90: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2902 - accuracy: 0.8870 - val_loss: 0.6413 - val_accuracy: 0.6909\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8522\n",
            "Epoch 91: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3213 - accuracy: 0.8522 - val_loss: 0.7137 - val_accuracy: 0.6727\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.8739\n",
            "Epoch 92: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2868 - accuracy: 0.8739 - val_loss: 0.6904 - val_accuracy: 0.6545\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9043\n",
            "Epoch 93: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2437 - accuracy: 0.9043 - val_loss: 0.7750 - val_accuracy: 0.6909\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.8935\n",
            "Epoch 94: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2674 - accuracy: 0.8935 - val_loss: 0.5457 - val_accuracy: 0.7636\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8674\n",
            "Epoch 95: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2767 - accuracy: 0.8674 - val_loss: 0.6876 - val_accuracy: 0.7091\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.8783\n",
            "Epoch 96: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2834 - accuracy: 0.8783 - val_loss: 0.7106 - val_accuracy: 0.7273\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.8826\n",
            "Epoch 97: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2935 - accuracy: 0.8826 - val_loss: 0.8753 - val_accuracy: 0.6545\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.8848\n",
            "Epoch 98: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2647 - accuracy: 0.8848 - val_loss: 0.6519 - val_accuracy: 0.6909\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9000\n",
            "Epoch 99: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2698 - accuracy: 0.9000 - val_loss: 0.7255 - val_accuracy: 0.7091\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.8891\n",
            "Epoch 100: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2556 - accuracy: 0.8891 - val_loss: 0.8259 - val_accuracy: 0.6909\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Assuming train_gen and valid_gen are your image data generators\n",
        "\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys()))\n",
        "\n",
        "# Create pre-trained model\n",
        "base_model = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=img_shape, pooling='max')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "     Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define filepath to save the best model\n",
        "filepath = 'best_model_resnet.h5'\n",
        "\n",
        "# Create ModelCheckpoint callback to save the best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "# Train the model with the added callback\n",
        "history = model.fit(\n",
        "    x=train_gen,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=valid_gen,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4e889d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:42:42.317878Z",
          "iopub.status.busy": "2024-01-20T18:42:42.317518Z",
          "iopub.status.idle": "2024-01-20T18:42:55.495749Z",
          "shell.execute_reply": "2024-01-20T18:42:55.494478Z"
        },
        "papermill": {
          "duration": 13.215101,
          "end_time": "2024-01-20T18:42:55.497738",
          "exception": false,
          "start_time": "2024-01-20T18:42:42.282637",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef4e889d",
        "outputId": "94dfe51e-04b7-470d-d43d-cd5ae3b8960a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 11s 1s/step - loss: 0.3554 - accuracy: 0.8435\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5392 - accuracy: 0.7273\n",
            "1/1 [==============================] - 1s 620ms/step - loss: 0.5732 - accuracy: 0.6667\n",
            "Train Loss:  0.3553749620914459\n",
            "Train Accuracy:  0.843478262424469\n",
            "--------------------\n",
            "Validation Loss:  0.5392179489135742\n",
            "Validation Accuracy:  0.7272727489471436\n",
            "--------------------\n",
            "Test Loss:  0.5731932520866394\n",
            "Test Accuracy:  0.6666666865348816\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "\n",
        "\n",
        "\n",
        "model= load_model('best_model_resnet.h5')\n",
        "\n",
        "\n",
        "train_score = model.evaluate(train_gen, steps= len(train_gen), verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= len(valid_gen), verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= len(test_gen), verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50890343",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:42:55.568921Z",
          "iopub.status.busy": "2024-01-20T18:42:55.568640Z",
          "iopub.status.idle": "2024-01-20T18:42:59.440582Z",
          "shell.execute_reply": "2024-01-20T18:42:59.439422Z"
        },
        "papermill": {
          "duration": 3.909692,
          "end_time": "2024-01-20T18:42:59.442858",
          "exception": false,
          "start_time": "2024-01-20T18:42:55.533166",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50890343",
        "outputId": "f8dabf3d-3819-4e64-b28d-d51b56684065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-edc74298ee48>:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  preds = model.predict_generator(test_gen)\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79b8c4204700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "model = load_model('best_model_resnet.h5')\n",
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9127df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:42:59.519785Z",
          "iopub.status.busy": "2024-01-20T18:42:59.519426Z",
          "iopub.status.idle": "2024-01-20T18:42:59.991250Z",
          "shell.execute_reply": "2024-01-20T18:42:59.990397Z"
        },
        "papermill": {
          "duration": 0.511496,
          "end_time": "2024-01-20T18:42:59.993429",
          "exception": false,
          "start_time": "2024-01-20T18:42:59.481933",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ab9127df",
        "outputId": "13507e2b-8f32-4081-cca6-89f5d8d19dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix, Without Normalization\n",
            "[[20 10]\n",
            " [10 20]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        nike       0.67      0.67      0.67        30\n",
            "      adidas       0.67      0.67      0.67        30\n",
            "\n",
            "    accuracy                           0.67        60\n",
            "   macro avg       0.67      0.67      0.67        60\n",
            "weighted avg       0.67      0.67      0.67        60\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc7220f",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-01-20T18:43:00.066242Z",
          "iopub.status.busy": "2024-01-20T18:43:00.065611Z",
          "iopub.status.idle": "2024-01-20T18:45:57.223032Z",
          "shell.execute_reply": "2024-01-20T18:45:57.222124Z"
        },
        "papermill": {
          "duration": 177.196261,
          "end_time": "2024-01-20T18:45:57.225108",
          "exception": false,
          "start_time": "2024-01-20T18:43:00.028847",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edc7220f",
        "outputId": "29184fa0-f7a1-4662-b86d-b7f5b7d951c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.9304 - accuracy: 0.5707"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - ETA: 0s - loss: 0.8778 - accuracy: 0.5935\n",
            "Epoch 1: val_accuracy improved from -inf to 0.50909, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.8778 - accuracy: 0.5935 - val_loss: 0.9313 - val_accuracy: 0.5091\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.7370\n",
            "Epoch 2: val_accuracy improved from 0.50909 to 0.54545, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.5804 - accuracy: 0.7370 - val_loss: 0.7765 - val_accuracy: 0.5455\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.7630\n",
            "Epoch 3: val_accuracy improved from 0.54545 to 0.58182, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.5346 - accuracy: 0.7630 - val_loss: 0.6477 - val_accuracy: 0.5818\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8109\n",
            "Epoch 4: val_accuracy improved from 0.58182 to 0.65455, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4318 - accuracy: 0.8109 - val_loss: 0.5621 - val_accuracy: 0.6545\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4033 - accuracy: 0.8217\n",
            "Epoch 5: val_accuracy did not improve from 0.65455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.4033 - accuracy: 0.8217 - val_loss: 0.6056 - val_accuracy: 0.6364\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8413\n",
            "Epoch 6: val_accuracy improved from 0.65455 to 0.76364, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3545 - accuracy: 0.8413 - val_loss: 0.5761 - val_accuracy: 0.7636\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.8565\n",
            "Epoch 7: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3227 - accuracy: 0.8565 - val_loss: 0.6182 - val_accuracy: 0.6909\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8717\n",
            "Epoch 8: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2964 - accuracy: 0.8717 - val_loss: 0.6218 - val_accuracy: 0.6727\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.8717\n",
            "Epoch 9: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3095 - accuracy: 0.8717 - val_loss: 0.5726 - val_accuracy: 0.6909\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.8783\n",
            "Epoch 10: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.3044 - accuracy: 0.8783 - val_loss: 0.6274 - val_accuracy: 0.6727\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8630\n",
            "Epoch 11: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2878 - accuracy: 0.8630 - val_loss: 0.5328 - val_accuracy: 0.7455\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.8913\n",
            "Epoch 12: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2390 - accuracy: 0.8913 - val_loss: 0.5773 - val_accuracy: 0.6727\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9022\n",
            "Epoch 13: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2283 - accuracy: 0.9022 - val_loss: 0.6279 - val_accuracy: 0.6727\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.8935\n",
            "Epoch 14: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2643 - accuracy: 0.8935 - val_loss: 0.6007 - val_accuracy: 0.6727\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9196\n",
            "Epoch 15: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2090 - accuracy: 0.9196 - val_loss: 0.5718 - val_accuracy: 0.7091\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.8761\n",
            "Epoch 16: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2474 - accuracy: 0.8761 - val_loss: 0.5622 - val_accuracy: 0.7091\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9065\n",
            "Epoch 17: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2056 - accuracy: 0.9065 - val_loss: 0.5708 - val_accuracy: 0.6727\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9196\n",
            "Epoch 18: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1955 - accuracy: 0.9196 - val_loss: 0.5578 - val_accuracy: 0.7636\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9196\n",
            "Epoch 19: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1906 - accuracy: 0.9196 - val_loss: 0.5093 - val_accuracy: 0.6909\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9065\n",
            "Epoch 20: val_accuracy did not improve from 0.76364\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2036 - accuracy: 0.9065 - val_loss: 0.5254 - val_accuracy: 0.7091\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9543\n",
            "Epoch 21: val_accuracy improved from 0.76364 to 0.81818, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1413 - accuracy: 0.9543 - val_loss: 0.4852 - val_accuracy: 0.8182\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9326\n",
            "Epoch 22: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1632 - accuracy: 0.9326 - val_loss: 0.5326 - val_accuracy: 0.7636\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9391\n",
            "Epoch 23: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1742 - accuracy: 0.9391 - val_loss: 0.5655 - val_accuracy: 0.7091\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9304\n",
            "Epoch 24: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.2032 - accuracy: 0.9304 - val_loss: 0.5444 - val_accuracy: 0.7273\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9565\n",
            "Epoch 25: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1347 - accuracy: 0.9565 - val_loss: 0.4300 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9457\n",
            "Epoch 26: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1428 - accuracy: 0.9457 - val_loss: 0.5623 - val_accuracy: 0.7273\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9478\n",
            "Epoch 27: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1351 - accuracy: 0.9478 - val_loss: 0.7594 - val_accuracy: 0.6909\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9435\n",
            "Epoch 28: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1461 - accuracy: 0.9435 - val_loss: 0.7344 - val_accuracy: 0.7273\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9522\n",
            "Epoch 29: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1461 - accuracy: 0.9522 - val_loss: 0.5823 - val_accuracy: 0.7818\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9522\n",
            "Epoch 30: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1212 - accuracy: 0.9522 - val_loss: 0.4922 - val_accuracy: 0.7636\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9391\n",
            "Epoch 31: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1454 - accuracy: 0.9391 - val_loss: 0.5337 - val_accuracy: 0.7636\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9348\n",
            "Epoch 32: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1702 - accuracy: 0.9348 - val_loss: 0.6730 - val_accuracy: 0.7273\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9565\n",
            "Epoch 33: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1115 - accuracy: 0.9565 - val_loss: 0.5498 - val_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9587\n",
            "Epoch 34: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1167 - accuracy: 0.9587 - val_loss: 0.4465 - val_accuracy: 0.7636\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9413\n",
            "Epoch 35: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1175 - accuracy: 0.9413 - val_loss: 0.4931 - val_accuracy: 0.8000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9348\n",
            "Epoch 36: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.1619 - accuracy: 0.9348 - val_loss: 0.7504 - val_accuracy: 0.7818\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9587\n",
            "Epoch 37: val_accuracy did not improve from 0.81818\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1112 - accuracy: 0.9587 - val_loss: 0.7084 - val_accuracy: 0.7273\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9478\n",
            "Epoch 38: val_accuracy improved from 0.81818 to 0.83636, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1273 - accuracy: 0.9478 - val_loss: 0.3804 - val_accuracy: 0.8364\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9630\n",
            "Epoch 39: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0918 - accuracy: 0.9630 - val_loss: 0.5313 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9739\n",
            "Epoch 40: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0716 - accuracy: 0.9739 - val_loss: 0.4388 - val_accuracy: 0.8182\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9543\n",
            "Epoch 41: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1177 - accuracy: 0.9543 - val_loss: 0.5236 - val_accuracy: 0.7636\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9696\n",
            "Epoch 42: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1067 - accuracy: 0.9696 - val_loss: 0.4607 - val_accuracy: 0.7818\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9674\n",
            "Epoch 43: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0884 - accuracy: 0.9674 - val_loss: 0.5091 - val_accuracy: 0.7091\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9565\n",
            "Epoch 44: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1136 - accuracy: 0.9565 - val_loss: 0.5419 - val_accuracy: 0.7818\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9674\n",
            "Epoch 45: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0903 - accuracy: 0.9674 - val_loss: 0.5871 - val_accuracy: 0.7818\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9565\n",
            "Epoch 46: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.1012 - accuracy: 0.9565 - val_loss: 0.5814 - val_accuracy: 0.7636\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9565\n",
            "Epoch 47: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1077 - accuracy: 0.9565 - val_loss: 0.6688 - val_accuracy: 0.7455\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9674\n",
            "Epoch 48: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0900 - accuracy: 0.9674 - val_loss: 0.7883 - val_accuracy: 0.7273\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9674\n",
            "Epoch 49: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0822 - accuracy: 0.9674 - val_loss: 0.7685 - val_accuracy: 0.7818\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9565\n",
            "Epoch 50: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0995 - accuracy: 0.9565 - val_loss: 0.5607 - val_accuracy: 0.8000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9609\n",
            "Epoch 51: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0820 - accuracy: 0.9609 - val_loss: 0.3821 - val_accuracy: 0.8000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9717\n",
            "Epoch 52: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.0753 - accuracy: 0.9717 - val_loss: 0.6075 - val_accuracy: 0.7818\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9652\n",
            "Epoch 53: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0925 - accuracy: 0.9652 - val_loss: 0.7581 - val_accuracy: 0.7273\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9739\n",
            "Epoch 54: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0754 - accuracy: 0.9739 - val_loss: 0.8778 - val_accuracy: 0.7091\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9761\n",
            "Epoch 55: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0714 - accuracy: 0.9761 - val_loss: 0.7719 - val_accuracy: 0.7636\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9761\n",
            "Epoch 56: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0681 - accuracy: 0.9761 - val_loss: 0.8636 - val_accuracy: 0.7818\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9565\n",
            "Epoch 57: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0960 - accuracy: 0.9565 - val_loss: 0.6515 - val_accuracy: 0.7636\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9696\n",
            "Epoch 58: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0959 - accuracy: 0.9696 - val_loss: 0.6972 - val_accuracy: 0.7455\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9696\n",
            "Epoch 59: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0724 - accuracy: 0.9696 - val_loss: 0.7396 - val_accuracy: 0.7455\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9587\n",
            "Epoch 60: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1102 - accuracy: 0.9587 - val_loss: 0.6624 - val_accuracy: 0.7636\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9674\n",
            "Epoch 61: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0901 - accuracy: 0.9674 - val_loss: 0.6766 - val_accuracy: 0.7636\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9674\n",
            "Epoch 62: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1025 - accuracy: 0.9674 - val_loss: 0.5505 - val_accuracy: 0.7455\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9696\n",
            "Epoch 63: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0788 - accuracy: 0.9696 - val_loss: 0.7889 - val_accuracy: 0.7455\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9696\n",
            "Epoch 64: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0892 - accuracy: 0.9696 - val_loss: 0.6658 - val_accuracy: 0.7455\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9652\n",
            "Epoch 65: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0828 - accuracy: 0.9652 - val_loss: 0.7547 - val_accuracy: 0.6909\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9739\n",
            "Epoch 66: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0659 - accuracy: 0.9739 - val_loss: 0.6088 - val_accuracy: 0.7818\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9674\n",
            "Epoch 67: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0863 - accuracy: 0.9674 - val_loss: 0.8706 - val_accuracy: 0.7455\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9696\n",
            "Epoch 68: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0908 - accuracy: 0.9696 - val_loss: 0.6173 - val_accuracy: 0.7091\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9739\n",
            "Epoch 69: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0600 - accuracy: 0.9739 - val_loss: 0.8673 - val_accuracy: 0.7273\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9565\n",
            "Epoch 70: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0991 - accuracy: 0.9565 - val_loss: 0.6645 - val_accuracy: 0.8182\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9717\n",
            "Epoch 71: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0709 - accuracy: 0.9717 - val_loss: 0.7958 - val_accuracy: 0.7636\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9761\n",
            "Epoch 72: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0777 - accuracy: 0.9761 - val_loss: 0.7943 - val_accuracy: 0.7091\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9717\n",
            "Epoch 73: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0737 - accuracy: 0.9717 - val_loss: 0.6393 - val_accuracy: 0.7818\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9609\n",
            "Epoch 74: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1125 - accuracy: 0.9609 - val_loss: 0.8083 - val_accuracy: 0.7818\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9674\n",
            "Epoch 75: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0780 - accuracy: 0.9674 - val_loss: 0.6418 - val_accuracy: 0.7636\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9848\n",
            "Epoch 76: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0588 - accuracy: 0.9848 - val_loss: 0.7003 - val_accuracy: 0.7455\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9717\n",
            "Epoch 77: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0613 - accuracy: 0.9717 - val_loss: 0.6640 - val_accuracy: 0.7273\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9826\n",
            "Epoch 78: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0558 - accuracy: 0.9826 - val_loss: 0.5287 - val_accuracy: 0.7818\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9739\n",
            "Epoch 79: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0572 - accuracy: 0.9739 - val_loss: 0.5844 - val_accuracy: 0.8000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9739\n",
            "Epoch 80: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0555 - accuracy: 0.9739 - val_loss: 0.6206 - val_accuracy: 0.7636\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9804\n",
            "Epoch 81: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0524 - accuracy: 0.9804 - val_loss: 0.8218 - val_accuracy: 0.7636\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9739\n",
            "Epoch 82: val_accuracy did not improve from 0.83636\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0661 - accuracy: 0.9739 - val_loss: 0.7697 - val_accuracy: 0.7273\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9783\n",
            "Epoch 83: val_accuracy improved from 0.83636 to 0.85455, saving model to best_model_vgg.h5\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0491 - accuracy: 0.9783 - val_loss: 0.5304 - val_accuracy: 0.8545\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9848\n",
            "Epoch 84: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0375 - accuracy: 0.9848 - val_loss: 0.5983 - val_accuracy: 0.8000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9739\n",
            "Epoch 85: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0507 - accuracy: 0.9739 - val_loss: 0.6763 - val_accuracy: 0.8000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9783\n",
            "Epoch 86: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0617 - accuracy: 0.9783 - val_loss: 0.7508 - val_accuracy: 0.8000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9848\n",
            "Epoch 87: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0529 - accuracy: 0.9848 - val_loss: 0.7407 - val_accuracy: 0.7273\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9761\n",
            "Epoch 88: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0492 - accuracy: 0.9761 - val_loss: 0.6893 - val_accuracy: 0.8000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9739\n",
            "Epoch 89: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0683 - accuracy: 0.9739 - val_loss: 0.5925 - val_accuracy: 0.7818\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9804\n",
            "Epoch 90: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.0775 - accuracy: 0.9804 - val_loss: 0.6128 - val_accuracy: 0.7636\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9674\n",
            "Epoch 91: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 2s/step - loss: 0.0724 - accuracy: 0.9674 - val_loss: 0.6631 - val_accuracy: 0.8182\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9674\n",
            "Epoch 92: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0686 - accuracy: 0.9674 - val_loss: 1.0089 - val_accuracy: 0.6727\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9696\n",
            "Epoch 93: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0801 - accuracy: 0.9696 - val_loss: 0.8271 - val_accuracy: 0.7818\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9761\n",
            "Epoch 94: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0637 - accuracy: 0.9761 - val_loss: 0.7644 - val_accuracy: 0.7091\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9761\n",
            "Epoch 95: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0656 - accuracy: 0.9761 - val_loss: 0.5877 - val_accuracy: 0.7455\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9804\n",
            "Epoch 96: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0436 - accuracy: 0.9804 - val_loss: 0.6529 - val_accuracy: 0.7636\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9783\n",
            "Epoch 97: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0581 - accuracy: 0.9783 - val_loss: 0.7493 - val_accuracy: 0.7636\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9696\n",
            "Epoch 98: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0810 - accuracy: 0.9696 - val_loss: 0.6621 - val_accuracy: 0.7273\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9674\n",
            "Epoch 99: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0687 - accuracy: 0.9674 - val_loss: 0.7175 - val_accuracy: 0.7636\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9696\n",
            "Epoch 100: val_accuracy did not improve from 0.85455\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0724 - accuracy: 0.9696 - val_loss: 0.6377 - val_accuracy: 0.7455\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Assuming train_gen and valid_gen are your image data generators\n",
        "\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys()))\n",
        "\n",
        "# Create pre-trained model\n",
        "base_model = tf.keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_shape=img_shape, pooling='max')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "     Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define filepath to save the best model\n",
        "filepath = 'best_model_vgg.h5'\n",
        "\n",
        "# Create ModelCheckpoint callback to save the best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "# Train the model with the added callback\n",
        "history = model.fit(\n",
        "    x=train_gen,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=valid_gen,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7785c24",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:45:57.324387Z",
          "iopub.status.busy": "2024-01-20T18:45:57.324039Z",
          "iopub.status.idle": "2024-01-20T18:46:07.991286Z",
          "shell.execute_reply": "2024-01-20T18:46:07.990393Z"
        },
        "papermill": {
          "duration": 10.718513,
          "end_time": "2024-01-20T18:46:07.993159",
          "exception": false,
          "start_time": "2024-01-20T18:45:57.274646",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7785c24",
        "outputId": "6b3434df-9852-4f0c-ad0c-2587e69aac8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 10s 1s/step - loss: 0.0203 - accuracy: 0.9935\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8785 - accuracy: 0.7455\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.8665 - accuracy: 0.7167\n",
            "Train Loss:  0.020303605124354362\n",
            "Train Accuracy:  0.9934782385826111\n",
            "--------------------\n",
            "Validation Loss:  0.8784945607185364\n",
            "Validation Accuracy:  0.7454545497894287\n",
            "--------------------\n",
            "Test Loss:  0.8664858341217041\n",
            "Test Accuracy:  0.7166666388511658\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "\n",
        "\n",
        "\n",
        "model= load_model('best_model_vgg.h5')\n",
        "\n",
        "\n",
        "train_score = model.evaluate(train_gen, steps= len(train_gen), verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= len(valid_gen), verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= len(test_gen), verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62efdbab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:46:08.092954Z",
          "iopub.status.busy": "2024-01-20T18:46:08.092645Z",
          "iopub.status.idle": "2024-01-20T18:46:09.310489Z",
          "shell.execute_reply": "2024-01-20T18:46:09.309395Z"
        },
        "papermill": {
          "duration": 1.269863,
          "end_time": "2024-01-20T18:46:09.312725",
          "exception": false,
          "start_time": "2024-01-20T18:46:08.042862",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62efdbab",
        "outputId": "e6b34f44-7c01-4c6c-c726-bd385ebad2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-192cfa0cd1d9>:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  preds = model.predict_generator(test_gen)\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79b850882f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "model = load_model('best_model_vgg.h5')\n",
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc51dae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T18:46:09.417532Z",
          "iopub.status.busy": "2024-01-20T18:46:09.416998Z",
          "iopub.status.idle": "2024-01-20T18:46:09.891870Z",
          "shell.execute_reply": "2024-01-20T18:46:09.890917Z"
        },
        "papermill": {
          "duration": 0.527989,
          "end_time": "2024-01-20T18:46:09.894082",
          "exception": false,
          "start_time": "2024-01-20T18:46:09.366093",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4cc51dae",
        "outputId": "546a8db4-a63d-4cf9-fa41-b58b324f9f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix, Without Normalization\n",
            "[[18 12]\n",
            " [ 5 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        nike       0.78      0.60      0.68        30\n",
            "      adidas       0.68      0.83      0.75        30\n",
            "\n",
            "    accuracy                           0.72        60\n",
            "   macro avg       0.73      0.72      0.71        60\n",
            "weighted avg       0.73      0.72      0.71        60\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= target_names))"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 2361804,
          "sourceId": 3980041,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 592.9947,
      "end_time": "2024-01-20T18:46:13.072719",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-01-20T18:36:20.078019",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}